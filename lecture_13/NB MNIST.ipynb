{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"../datasets/mnist_train_small.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data[:, 1:]/255, data[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.824391219560978"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X, y)\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Naive Bayes classifier for multinomial models\n",
       "\n",
       "The multinomial Naive Bayes classifier is suitable for classification with\n",
       "discrete features (e.g., word counts for text classification). The\n",
       "multinomial distribution normally requires integer feature counts. However,\n",
       "in practice, fractional counts such as tf-idf may also work.\n",
       "\n",
       "Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "alpha : float, optional (default=1.0)\n",
       "    Additive (Laplace/Lidstone) smoothing parameter\n",
       "    (0 for no smoothing).\n",
       "\n",
       "fit_prior : boolean, optional (default=True)\n",
       "    Whether to learn class prior probabilities or not.\n",
       "    If false, a uniform prior will be used.\n",
       "\n",
       "class_prior : array-like, size (n_classes,), optional (default=None)\n",
       "    Prior probabilities of the classes. If specified the priors are not\n",
       "    adjusted according to the data.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "class_log_prior_ : array, shape (n_classes, )\n",
       "    Smoothed empirical log probability for each class.\n",
       "\n",
       "intercept_ : array, shape (n_classes, )\n",
       "    Mirrors ``class_log_prior_`` for interpreting MultinomialNB\n",
       "    as a linear model.\n",
       "\n",
       "feature_log_prob_ : array, shape (n_classes, n_features)\n",
       "    Empirical log probability of features\n",
       "    given a class, ``P(x_i|y)``.\n",
       "\n",
       "coef_ : array, shape (n_classes, n_features)\n",
       "    Mirrors ``feature_log_prob_`` for interpreting MultinomialNB\n",
       "    as a linear model.\n",
       "\n",
       "class_count_ : array, shape (n_classes,)\n",
       "    Number of samples encountered for each class during fitting. This\n",
       "    value is weighted by the sample weight when provided.\n",
       "\n",
       "feature_count_ : array, shape (n_classes, n_features)\n",
       "    Number of samples encountered for each (class, feature)\n",
       "    during fitting. This value is weighted by the sample weight when\n",
       "    provided.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> X = np.random.randint(5, size=(6, 100))\n",
       ">>> y = np.array([1, 2, 3, 4, 5, 6])\n",
       ">>> from sklearn.naive_bayes import MultinomialNB\n",
       ">>> clf = MultinomialNB()\n",
       ">>> clf.fit(X, y)\n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
       ">>> print(clf.predict(X[2:3]))\n",
       "[3]\n",
       "\n",
       "Notes\n",
       "-----\n",
       "For the rationale behind the names `coef_` and `intercept_`, i.e.\n",
       "naive Bayes as a linear classifier, see J. Rennie et al. (2003),\n",
       "Tackling the poor assumptions of naive Bayes text classifiers, ICML.\n",
       "\n",
       "References\n",
       "----------\n",
       "C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
       "Information Retrieval. Cambridge University Press, pp. 234-265.\n",
       "http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
       "\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
