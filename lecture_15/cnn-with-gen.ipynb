{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Conv2D, MaxPool2D, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import load_img\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = np.load(\"../datasets/mnist_train_small.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[:, 1:].reshape(-1, 28, 28, 1), mnist[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(categories=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hot = ohe.fit_transform(y.reshape(-1, 1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(zoom_range=.2, shear_range=.1, ).flow(X, y_hot, batch_size=320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "in_layer = Input((28,28, 1))\n",
    "conv1 = Conv2D(16, (3, 3))(in_layer)\n",
    "pool1 = MaxPool2D()(conv1)\n",
    "conv2 = Conv2D(32, (3, 3))(pool1)\n",
    "pool2 = MaxPool2D()(conv2)\n",
    "flat = Flatten()(pool2)\n",
    "d1 = Dense(400, activation=\"relu\")(flat)\n",
    "drop1 = Dropout(rate=0.25)(d1)\n",
    "d2 = Dense(200, activation=\"relu\")(drop1)\n",
    "drop2 = Dropout(rate=0.25)(d2)\n",
    "d3 = Dense(100, activation=\"relu\")(drop2)\n",
    "out = Dense(10, activation=\"softmax\")(d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[in_layer], outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 400)               320400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 426,510\n",
      "Trainable params: 426,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 6s 117ms/step - loss: 9.7004 - acc: 0.3790\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 5s 110ms/step - loss: 4.2368 - acc: 0.7018\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 5s 107ms/step - loss: 1.4191 - acc: 0.8339\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 5s 106ms/step - loss: 0.3594 - acc: 0.9104\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 5s 107ms/step - loss: 0.2638 - acc: 0.9301\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 5s 110ms/step - loss: 0.2094 - acc: 0.9447\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 5s 107ms/step - loss: 0.1763 - acc: 0.9492\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 5s 109ms/step - loss: 0.1572 - acc: 0.9569\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 5s 108ms/step - loss: 0.1478 - acc: 0.9586\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 6s 112ms/step - loss: 0.1284 - acc: 0.9629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1330293c8>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(gen, steps_per_epoch=50, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06013865619897842, 0.9840624988079071]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(gen, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6600"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.0274258e-06, 1.1827090e-06, 4.0759690e-04, 7.7223187e-05,\n",
       "        8.2360503e-07, 5.3463776e-08, 6.7182229e-09, 9.9876213e-01,\n",
       "        1.9856230e-05, 7.2717562e-04],\n",
       "       [1.1017884e-06, 2.5717904e-08, 1.3354450e-07, 1.3133698e-07,\n",
       "        5.9759148e-10, 8.5169198e-12, 1.5963362e-13, 9.9995959e-01,\n",
       "        1.6780218e-08, 3.9047845e-05],\n",
       "       [9.9512297e-01, 4.6875433e-14, 1.3542972e-05, 1.0193923e-11,\n",
       "        1.3461229e-09, 3.2059049e-12, 2.3970909e-11, 2.3060887e-09,\n",
       "        2.2179650e-08, 4.8634307e-03],\n",
       "       [2.3381426e-09, 1.6913283e-12, 4.6870428e-11, 2.1959067e-10,\n",
       "        1.8354015e-08, 1.1054182e-11, 3.3995398e-12, 3.5284400e-09,\n",
       "        1.9688279e-08, 1.0000000e+00],\n",
       "       [2.2745375e-12, 2.3474411e-09, 8.3529778e-10, 7.8244564e-12,\n",
       "        9.9992347e-01, 6.8746883e-12, 6.7229685e-13, 8.9004379e-06,\n",
       "        7.9968494e-08, 6.7457760e-05],\n",
       "       [4.4193656e-11, 2.8456729e-14, 7.8708004e-15, 8.7813099e-09,\n",
       "        3.4330037e-09, 9.9999976e-01, 6.2187291e-12, 9.4342889e-10,\n",
       "        1.5170440e-10, 2.5559993e-07],\n",
       "       [9.9600205e-14, 4.3469347e-12, 8.0243407e-12, 4.1949869e-14,\n",
       "        9.9998701e-01, 1.1612827e-11, 6.3503760e-13, 5.9827225e-06,\n",
       "        1.1689536e-08, 7.0754018e-06],\n",
       "       [2.1213452e-07, 2.0064052e-14, 6.9866235e-07, 6.6870980e-16,\n",
       "        2.9806297e-05, 7.7769700e-14, 9.9996924e-01, 1.3058207e-12,\n",
       "        5.6566362e-12, 3.7171564e-12],\n",
       "       [7.8892427e-11, 1.9188802e-13, 1.4818077e-11, 4.9727514e-11,\n",
       "        2.7580779e-07, 2.3153065e-15, 3.6772817e-14, 1.3515800e-09,\n",
       "        6.7723195e-11, 9.9999976e-01],\n",
       "       [3.7409973e-13, 1.1614413e-08, 1.0000000e+00, 2.7523070e-08,\n",
       "        2.3382883e-14, 2.3034168e-13, 2.7526797e-08, 2.9221916e-12,\n",
       "        9.6667681e-11, 1.0319801e-14]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'matrix' object has no attribute 'todense'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-352f2233cb0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'matrix' object has no attribute 'todense'"
     ]
    }
   ],
   "source": [
    "y_test[:10].todense().argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 7, 3, 4, 1, 9, 2, 6, 8, 0]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = load_img(\"../datasets/nums/six.jpeg\", target_size=(28, 28), grayscale=True)\n",
    "plane = (255 - np.array(img)).astype(int)\n",
    "plt.imshow(plane, cmap=\"gray\")\n",
    "model.predict(np.array([plane.reshape(28, 28, 1)])).argsort(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m[\u001b[0m\u001b[0;34m'featurewise_center=False'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'samplewise_center=False'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'featurewise_std_normalization=False'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'samplewise_std_normalization=False'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'zca_whitening=False'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'zca_epsilon=1e-06'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rotation_range=0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'width_shift_range=0.0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'height_shift_range=0.0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'brightness_range=None'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shear_range=0.0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'zoom_range=0.0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'channel_shift_range=0.0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fill_mode='nearest'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cval=0.0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'horizontal_flip=False'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vertical_flip=False'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rescale=None'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'preprocessing_function=None'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data_format=None'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validation_split=0.0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtype=None'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Generate batches of tensor image data with real-time data augmentation.\n",
       " The data will be looped over (in batches).\n",
       "\n",
       "# Arguments\n",
       "    featurewise_center: Boolean.\n",
       "        Set input mean to 0 over the dataset, feature-wise.\n",
       "    samplewise_center: Boolean. Set each sample mean to 0.\n",
       "    featurewise_std_normalization: Boolean.\n",
       "        Divide inputs by std of the dataset, feature-wise.\n",
       "    samplewise_std_normalization: Boolean. Divide each input by its std.\n",
       "    zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n",
       "    zca_whitening: Boolean. Apply ZCA whitening.\n",
       "    rotation_range: Int. Degree range for random rotations.\n",
       "    width_shift_range: Float, 1-D array-like or int\n",
       "        - float: fraction of total width, if < 1, or pixels if >= 1.\n",
       "        - 1-D array-like: random elements from the array.\n",
       "        - int: integer number of pixels from interval\n",
       "            `(-width_shift_range, +width_shift_range)`\n",
       "        - With `width_shift_range=2` possible values\n",
       "            are integers `[-1, 0, +1]`,\n",
       "            same as with `width_shift_range=[-1, 0, +1]`,\n",
       "            while with `width_shift_range=1.0` possible values are floats\n",
       "            in the interval [-1.0, +1.0).\n",
       "    height_shift_range: Float, 1-D array-like or int\n",
       "        - float: fraction of total height, if < 1, or pixels if >= 1.\n",
       "        - 1-D array-like: random elements from the array.\n",
       "        - int: integer number of pixels from interval\n",
       "            `(-height_shift_range, +height_shift_range)`\n",
       "        - With `height_shift_range=2` possible values\n",
       "            are integers `[-1, 0, +1]`,\n",
       "            same as with `height_shift_range=[-1, 0, +1]`,\n",
       "            while with `height_shift_range=1.0` possible values are floats\n",
       "            in the interval [-1.0, +1.0).\n",
       "    brightness_range: Tuple or list of two floats. Range for picking\n",
       "        a brightness shift value from.\n",
       "    shear_range: Float. Shear Intensity\n",
       "        (Shear angle in counter-clockwise direction in degrees)\n",
       "    zoom_range: Float or [lower, upper]. Range for random zoom.\n",
       "        If a float, `[lower, upper] = [1-zoom_range, 1+zoom_range]`.\n",
       "    channel_shift_range: Float. Range for random channel shifts.\n",
       "    fill_mode: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}.\n",
       "        Default is 'nearest'.\n",
       "        Points outside the boundaries of the input are filled\n",
       "        according to the given mode:\n",
       "        - 'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n",
       "        - 'nearest':  aaaaaaaa|abcd|dddddddd\n",
       "        - 'reflect':  abcddcba|abcd|dcbaabcd\n",
       "        - 'wrap':  abcdabcd|abcd|abcdabcd\n",
       "    cval: Float or Int.\n",
       "        Value used for points outside the boundaries\n",
       "        when `fill_mode = \"constant\"`.\n",
       "    horizontal_flip: Boolean. Randomly flip inputs horizontally.\n",
       "    vertical_flip: Boolean. Randomly flip inputs vertically.\n",
       "    rescale: rescaling factor. Defaults to None.\n",
       "        If None or 0, no rescaling is applied,\n",
       "        otherwise we multiply the data by the value provided\n",
       "        (after applying all other transformations).\n",
       "    preprocessing_function: function that will be implied on each input.\n",
       "        The function will run after the image is resized and augmented.\n",
       "        The function should take one argument:\n",
       "        one image (Numpy tensor with rank 3),\n",
       "        and should output a Numpy tensor with the same shape.\n",
       "    data_format: Image data format,\n",
       "        either \"channels_first\" or \"channels_last\".\n",
       "        \"channels_last\" mode means that the images should have shape\n",
       "        `(samples, height, width, channels)`,\n",
       "        \"channels_first\" mode means that the images should have shape\n",
       "        `(samples, channels, height, width)`.\n",
       "        It defaults to the `image_data_format` value found in your\n",
       "        Keras config file at `~/.keras/keras.json`.\n",
       "        If you never set it, then it will be \"channels_last\".\n",
       "    validation_split: Float. Fraction of images reserved for validation\n",
       "        (strictly between 0 and 1).\n",
       "    dtype: Dtype to use for the generated arrays.\n",
       "\n",
       "# Examples\n",
       "Example of using `.flow(x, y)`:\n",
       "\n",
       "```python\n",
       "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
       "y_train = np_utils.to_categorical(y_train, num_classes)\n",
       "y_test = np_utils.to_categorical(y_test, num_classes)\n",
       "\n",
       "datagen = ImageDataGenerator(\n",
       "    featurewise_center=True,\n",
       "    featurewise_std_normalization=True,\n",
       "    rotation_range=20,\n",
       "    width_shift_range=0.2,\n",
       "    height_shift_range=0.2,\n",
       "    horizontal_flip=True)\n",
       "\n",
       "# compute quantities required for featurewise normalization\n",
       "# (std, mean, and principal components if ZCA whitening is applied)\n",
       "datagen.fit(x_train)\n",
       "\n",
       "# fits the model on batches with real-time data augmentation:\n",
       "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
       "                    steps_per_epoch=len(x_train) / 32, epochs=epochs)\n",
       "\n",
       "# here's a more \"manual\" example\n",
       "for e in range(epochs):\n",
       "    print('Epoch', e)\n",
       "    batches = 0\n",
       "    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):\n",
       "        model.fit(x_batch, y_batch)\n",
       "        batches += 1\n",
       "        if batches >= len(x_train) / 32:\n",
       "            # we need to break the loop by hand because\n",
       "            # the generator loops indefinitely\n",
       "            break\n",
       "```\n",
       "Example of using `.flow_from_directory(directory)`:\n",
       "\n",
       "```python\n",
       "train_datagen = ImageDataGenerator(\n",
       "        rescale=1./255,\n",
       "        shear_range=0.2,\n",
       "        zoom_range=0.2,\n",
       "        horizontal_flip=True)\n",
       "\n",
       "test_datagen = ImageDataGenerator(rescale=1./255)\n",
       "\n",
       "train_generator = train_datagen.flow_from_directory(\n",
       "        'data/train',\n",
       "        target_size=(150, 150),\n",
       "        batch_size=32,\n",
       "        class_mode='binary')\n",
       "\n",
       "validation_generator = test_datagen.flow_from_directory(\n",
       "        'data/validation',\n",
       "        target_size=(150, 150),\n",
       "        batch_size=32,\n",
       "        class_mode='binary')\n",
       "\n",
       "model.fit_generator(\n",
       "        train_generator,\n",
       "        steps_per_epoch=2000,\n",
       "        epochs=50,\n",
       "        validation_data=validation_generator,\n",
       "        validation_steps=800)\n",
       "```\n",
       "\n",
       "Example of transforming images and masks together.\n",
       "\n",
       "```python\n",
       "# we create two instances with the same arguments\n",
       "data_gen_args = dict(featurewise_center=True,\n",
       "                     featurewise_std_normalization=True,\n",
       "                     rotation_range=90,\n",
       "                     width_shift_range=0.1,\n",
       "                     height_shift_range=0.1,\n",
       "                     zoom_range=0.2)\n",
       "image_datagen = ImageDataGenerator(**data_gen_args)\n",
       "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
       "\n",
       "# Provide the same seed and keyword arguments to the fit and flow methods\n",
       "seed = 1\n",
       "image_datagen.fit(images, augment=True, seed=seed)\n",
       "mask_datagen.fit(masks, augment=True, seed=seed)\n",
       "\n",
       "image_generator = image_datagen.flow_from_directory(\n",
       "    'data/images',\n",
       "    class_mode=None,\n",
       "    seed=seed)\n",
       "\n",
       "mask_generator = mask_datagen.flow_from_directory(\n",
       "    'data/masks',\n",
       "    class_mode=None,\n",
       "    seed=seed)\n",
       "\n",
       "# combine generators into one which yields image and masks\n",
       "train_generator = zip(image_generator, mask_generator)\n",
       "\n",
       "model.fit_generator(\n",
       "    train_generator,\n",
       "    steps_per_epoch=2000,\n",
       "    epochs=50)\n",
       "```\n",
       "\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.7/site-packages/keras/preprocessing/image.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ImageDataGenerator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
